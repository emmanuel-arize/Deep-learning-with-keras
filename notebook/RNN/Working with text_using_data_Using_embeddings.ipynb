{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Word-embeddings\" data-toc-modified-id=\"Word-embeddings-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Word embeddings</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to perform machine learning on text documents, the raw (text) data cannot be fed directly to algorithm as these algorithms expect numerical feature vectors so instead we need to turn the text content into numerical feature vectors.\n",
    "\n",
    "From the [scikit-learn documentation](https://scikit-learn.org/stable/modules/feature_extraction.html):\n",
    "<b>\n",
    "We call vectorization the general process of turning a collection of text documents into numerical feature vectors.\n",
    "</b>\n",
    "So Vectorizing text is the process of transforming text into numeric tensors. \n",
    "\n",
    "Vectorization can be done in multiple ways:\n",
    "- Segment text into words, and transform each word into a vector.\n",
    "- Segment text into characters, and transform each character into a vector.\n",
    "- Extract n-grams of words or characters, and transform each n-gram into a vector.\n",
    "N-grams are overlapping groups of multiple consecutive words or characters\n",
    "\n",
    "<b>Tokenization: </b> the segementation of text into words or characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Text-vectorization processes consist of applying some tokenization scheme to the text then associating numeric vectors with the generated tokens.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-hot encoding of words and characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = ['My team Bayern lost today','good boy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "characters = string.printable\n",
    "class OneHot_w:\n",
    "    def __init__(self,max_lenght):\n",
    "        self.max_lenght=max_lenght \n",
    "        \n",
    "    def Word_index(self,sentences):\n",
    "        word_index={}\n",
    "        for sent in sentences:\n",
    "            for word in sent.split():\n",
    "                if word not in word_index:\n",
    "                    word_index[word]=len(word_index)+1            \n",
    "        return word_index\n",
    "    \n",
    "    def __call__(self,sentences):\n",
    "        token_index=self.Word_index(sentences)\n",
    "        results=np.zeros((len(sentences),self.max_lenght,max(token_index.values())+1))\n",
    "        for i,sentence in enumerate(sentences):\n",
    "                for j ,word in list(enumerate(sentence.split()))[:self.max_lenght]:\n",
    "                    index=token_index.get(word)\n",
    "                    results[i,j,index]=1\n",
    "   \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oh=OneHot_w(max_lenght=6)\n",
    "oh(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# character encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = ['My team Bayern lost today','good boy']\n",
    "characters = string.printable\n",
    "token_index = dict(zip(range(1, len(characters) + 1), characters))\n",
    "max_length = 25\n",
    "results = np.zeros((len(samples), max_length, max(token_index.keys()) + 1))\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, character in enumerate(sample):\n",
    "        index = token_index.get(character)\n",
    "        results[i, j, index] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.]],\n",
       "\n",
       "       [[1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = ['My team Bayern lost today','good boy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer(num_words=10)\n",
    "tokenizer.fit_on_texts(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'my', 2: 'team', 3: 'bayern', 4: 'lost', 5: 'today', 6: 'good', 7: 'boy'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.document_count,tokenizer.num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 1., 0., 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq=tokenizer.texts_to_sequences(samples)\n",
    "one_hot=tokenizer.sequences_to_matrix(seq)\n",
    "one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 1., 0., 0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_matrix(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONE HOT CHARACTER ENCODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=Tokenizer(char_level=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens.fit_on_texts(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: ' ',\n",
       " 2: 'o',\n",
       " 3: 'y',\n",
       " 4: 't',\n",
       " 5: 'a',\n",
       " 6: 'm',\n",
       " 7: 'e',\n",
       " 8: 'b',\n",
       " 9: 'd',\n",
       " 10: 'r',\n",
       " 11: 'n',\n",
       " 12: 'l',\n",
       " 13: 's',\n",
       " 14: 'g'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "       [0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqq=tokens.texts_to_sequences(samples)\n",
    "tokens.sequences_to_matrix(seqq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another popular and powerful way to associate a vector with a word is the use of dense\n",
    "word vectors, also called word embeddings. Unlike the one-hot encoding, word embeddings are learned from data.\n",
    "\n",
    "\n",
    "<P>Consider the sentence \"The cat sat on the mat\". The vocabulary (or unique words) in this sentence is (cat, mat, on, sat, the). </P>\n",
    "From <a href=\"https://www.tensorflow.org/tutorials/text/word_embeddings\">Tensorflow documentation-Word embeddings</a>\n",
    "\n",
    "### Word embeddings\n",
    "\n",
    "> Word embeddings give us a way to use an efficient, dense representation in which similar words have a similar encoding. Importantly, you do not have to specify this encoding by hand. An embedding is a dense vector of floating point values (the length of the vector is a parameter you specify). Instead of specifying the values for the embedding manually, they are trainable parameters (weights learned by the model during training, in the same way a model learns weights for a dense layer). It is common to see word embeddings that are 8-dimensional (for small datasets), up to 1024-dimensions when working with large datasets. A higher dimensional embedding can capture fine-grained relationships between words, but takes more data to learn.\n",
    "<img src=\"images/embedding2.png?raw=1\" alt=\"Diagram of an embedding\" width=\"400\"/>\n",
    "Above is a diagram for a word embedding. Each word is represented as a 4-dimensional vector of floating point values. Another way to think of an embedding is as \"lookup table\". After these weights have been learned, you can encode each word by looking up the dense vector it corresponds to in the table.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# The Embedding layer\n",
    "From <a href=\"https://www.tensorflow.org/tutorials/text/word_embeddings\">Tensorflow documentation-Word embeddings</a>\n",
    "> Keras makes it easy to use word embeddings. Take a look at the Embedding layer.\n",
    "The Embedding layer can be understood as a lookup table that maps from integer indices (which stand for specific words) to dense vectors (their embeddings). The dimensionality (or width) of the embedding is a parameter you can experiment with to see what works well for your problem, much in the same way you would experiment with the number of neurons in a Dense layer.\n",
    "\n",
    "> When you create an Embedding layer, the weights for the embedding are randomly initialized (just like any other layer). During training, they are gradually adjusted via backpropagation. Once trained, the learned word embeddings will roughly encode similarities between words (as they were learned for the specific problem your model is trained on). If you pass an integer to an embedding layer, the result replaces each integer with the vector from the embedding table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding,Flatten,Dense,Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.datasets import imdb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For text or sequence problems, the Embedding layer takes as input a 2D tensor of integers, of shape (samples,sequence_length) and returns a 3D floating-point tensor of shape (samples, sequence_length, embedding_dimensionality). All sequences in a batch must have the same length, though (because you need to pack them into a single tensor), so sequences that are shorter than others should be padded with zeros, and sequences that are longer should be truncated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer=Embedding(10,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.00124159,  0.02829922,  0.02806177, -0.01132657,\n",
       "          0.01436049],\n",
       "        [ 0.02277093,  0.00207782,  0.00144293,  0.03614566,\n",
       "          0.01122695]],\n",
       "\n",
       "       [[ 0.02277093,  0.00207782,  0.00144293,  0.03614566,\n",
       "          0.01122695],\n",
       "        [ 0.0148036 , -0.01145764, -0.02028   ,  0.00362878,\n",
       "         -0.03939173]]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = embedding_layer(tf.constant([[1,2],[2,4]]))\n",
    "result.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the IMDB data for use with an Embedding layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Youâ€™ll restrict the movie reviews to the top 15,000 most common words and  considering looking at the first 30 words in every review. The network will learn 16-dimensional embeddings for each of the 15,000 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep the 15000 most common words\n",
    "max_features=15000\n",
    "embedding_dim=16\n",
    "max_length=30\n",
    "(x_train, y_train), (x_test, y_test)=imdb.load_data(num_words=max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_pad=pad_sequences(x_train,maxlen=max_length,padding='pre',truncating='pre')\n",
    "x_test_pad=pad_sequences(x_test,maxlen=max_length,padding='pre',truncating='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  49,  238,   60,  135, 1162,   14,    9,  290,    4,   58,   10,\n",
       "         10,  472,   45,   55,  878,    8,  169,   11,  374, 5687,   25,\n",
       "        203,   28,    8,  818,   12,  125,    4, 3077])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_pad[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 371,   78,   22,  625,   64, 1382,    9,    8,  168,  145,   23,\n",
       "          4, 1690,   15,   16,    4, 1355,    5,   28,    6,   52,  154,\n",
       "        462,   33,   89,   78,  285,   16,  145,   95])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_pad[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Embedding(input_dim=max_features,output_dim=embedding_dim,input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(14,activation='relu'))\n",
    "model.add(Dropout(0.05))\n",
    "model.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 30, 16)            240000    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 480)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 14)                6734      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 14)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 15        \n",
      "=================================================================\n",
      "Total params: 246,749\n",
      "Trainable params: 246,749\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "625/625 [==============================] - 4s 4ms/step - loss: 0.6625 - acc: 0.5893 - val_loss: 0.4837 - val_acc: 0.7618\n",
      "Epoch 2/6\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.4137 - acc: 0.8152 - val_loss: 0.4584 - val_acc: 0.7772\n",
      "Epoch 3/6\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.3253 - acc: 0.8615 - val_loss: 0.4723 - val_acc: 0.7786\n",
      "Epoch 4/6\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2655 - acc: 0.8924 - val_loss: 0.5150 - val_acc: 0.7736\n",
      "Epoch 5/6\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.1895 - acc: 0.9295 - val_loss: 0.5726 - val_acc: 0.7652\n",
      "Epoch 6/6\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.1239 - acc: 0.9598 - val_loss: 0.6645 - val_acc: 0.7556\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train_pad, y_train,epochs=6,batch_size=32,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
