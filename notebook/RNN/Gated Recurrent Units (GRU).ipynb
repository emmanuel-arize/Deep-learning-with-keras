{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gated Recurrent Units (GRU)\n",
    "\n",
    "In a coventional <a href='https://emmanuel-arize.github.io/datascience-blog/deeplearning/deep-learning/2021/05/06/RNN.html' target=\"_blank\">  recurrent neural network</a>, during the backpropagation phase, in which the error signal (gradients) are backpropagated through time, the recurrent hidden layers (weight matrix associated with the layers) are subject to repeated multiplications as determined by as the number of timesteps (length of the sequence), and this might result in numerical instability for lengthy sequence. For lengthy sequence, small weights tends to lead to a situation known as <b>vanishing gradients</b> \n",
    "where the error signal propagating backwards gets so small that learning either becomes very slow or stops working altogether (error signals fowing backwards in time tend to vanish). <b>Conversely </b>larger weights tends to lead to a situation where the error signal is so large that it can cause learning to diverge , a situation known as <b>exploding gradients</b>.\n",
    "\n",
    "To read more on exploding and vanishing gradients have a look at this papers\n",
    "<br/>\n",
    "<a href='https://arxiv.org/pdf/1211.5063v1.pdf' target=\"_blank\">Understanding the exploding gradient problem</a><br/>\n",
    "<a href='https://www.semanticscholar.org/paper/Learning-long-term-dependencies-with-gradient-is-Bengio-Simard/d0be39ee052d246ae99c082a565aba25b811be2d' target=\"_blank\">Learning long-term dependencies with gradient descent is difficult</a><br/> \n",
    "\n",
    "<a href='https://www.bioinf.jku.at/publications/older/2304.pdf' target=\"_blank\">THE VANISHING GRADIENT PROBLEM DURING LEARNING RECURRENT NEURAL NETS AND PROBLEM SOLUTIONS</a><br/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The vanishing and exploding gradients problem, limit the ability of conventional RNNs in modeling sequences with long range contextual dependencies and to address these issues, more complex network architectures known as Gated Neural Networks (GNNs) have been designed to help mitigate this problem by introducing “gates”  to control the flow of information into and out of the  network layers. There are several GNNs but in this tutorial we were learn about a notable example  known as <a href='https://arxiv.org/pdf/1406.1078v3.pdf' target='_blank'>Gated Recurrent Unit or GRU (Cho et al., 2014)</a>) which is similar to LSTM but with fewer parameters than LSTM, as it lacks an output gate and faster to train due to the simpler architecture. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img id='GRU' src=\"./images/GRU.png\" /><span id='GRU'>Figure 1</span>\n",
    "<a href='https://en.wikipedia.org/wiki/File:Gated_Recurrent_Unit,_base_type.svg'>Source</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From <a href='#GRU'>Figure 1</a> ***GRU has two gates, a reset $(r_{t})$ and update gates $(z_{t})$ ***. Tthe reset gate determines how to combine the new input with the previous hidden state. let assume we have a minibatch of inputs $X_{t} \\in R^{n×d}$ where each row of $X_{t}$ corresponds to one example at time step ***t*** from the sequence and the hidden state of the previous time step as $h_{t−1} \\in R^{n×h}$. Given an input, the first step of the GRU model is for the reset gate to decide whether to ignore the previous hidden state or not. With a reset gate value close to 0, the previous hidden state is dimmed irrelevant and the hidden state is forced to ignore the previous hidden state and reset with the current input. The reset gate is defined as\n",
    "\n",
    "\n",
    "$$ r_{t}=\\sigma(W_{xr}X_{t}+U_{hr}h_{t-1}+b_{r} )$$\n",
    "\n",
    "where $W_{xr}$, $U_{hr}$ are weight paramaters, $b_r$ the bias term and $\\sigma$ is the sigmoid activation function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ***update gate*** is defined as\n",
    "\n",
    "$$ z_{t}=\\sigma(W_{xz}X_{t}+U_{hz}h_{t-1}+b_{z})$$\n",
    "\n",
    "and it controls how information from the previous hidden state are carried over to the current hidden state\n",
    "\n",
    "Let now examine how the reset and update gates are integrated into the hidden state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The Hidden State</b> is computed by\n",
    "$$ h_{t}=z_{t} \\odot h_{t-1}+ (1-z_{t})\\odot \\bar h_{t} $$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\bar h_{t}=\\phi(W_{h}X_{t}+U_{h}(r_{t} \\odot h_{t-1})+b_{z})$$\n",
    "\n",
    "is known as the ***Candidate Hidden State*** , the operator $\\odot$ denotes the Hadamard product and the update gate **$z_t$** decides whether the hidden state is to be updated with\n",
    "the new Candidate Hidden State $\\bar h$  . \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ z_{t}=\\sigma(W_{xz}X_{t}+U_{hz}h_{t-1}+b_{z} \\rightarrow update \\ gate \\ vector$$\n",
    "\n",
    "\n",
    "$$ r_{t}=\\sigma(W_{xr}X_{t}+U_{hr}h_{t-1}+b_{r} \\rightarrow reset \\ gate \\ vector $$\n",
    "\n",
    "\n",
    "\n",
    "$$\\bar h_{t}=\\phi(W_{h}X_{t}+U_{h}(r_{t} \\odot h_{t-1})+b_{z}) \\rightarrow candidate\\ hidden\\ state$$\n",
    "\n",
    "\n",
    "$$ h_{t}=z_{t} \\odot h_{t-1}+ (1-z_{t})\\odot \\bar h_{t} \\rightarrow hidden \\ state $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multi-class classification on Stack Overflow questions\n",
    "This tutorial showed how to train a multi-class classifier to predict the tag of a programming question on Stack Overflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment to download the data\n",
    "# url='http://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz'\n",
    "# dataset=tf.keras.utils.get_file('stack_overflow',origin=url,untar=True,cache_dir='./data',\n",
    "#                                 cache_subdir='stackoverflow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dir(data):\n",
    "    dataset_dir=os.path.join(os.path.dirname('.'),data)\n",
    "    stackoverflow=os.path.join(os.path.dirname(dataset_dir),'stackoverflow/')\n",
    "    train_dir=os.path.join(os.path.dirname(stackoverflow),'train')\n",
    "    test_dir=os.path.join(os.path.dirname(stackoverflow),'test')\n",
    "    return train_dir, test_dir    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir,test_dir=load_dir('data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['csharp', 'java', 'javascript', 'python'],\n",
       " ['csharp', 'java', 'javascript', 'python'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(train_dir),os.listdir(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 files belonging to 4 classes.\n",
      "Using 6800 files for training.\n",
      "Found 8000 files belonging to 4 classes.\n",
      "Using 1200 files for validation.\n",
      "Found 8000 files belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size=100\n",
    "train_data=preprocessing.text_dataset_from_directory(directory=train_dir,subset='training',\n",
    "                                                    validation_split=0.15,batch_size=batch_size,\n",
    "                                                    seed=20)\n",
    "batch_size=10\n",
    "val_data=preprocessing.text_dataset_from_directory(directory=train_dir,subset='validation',\n",
    "                                                    validation_split=0.15,seed=20\n",
    "                                                     )\n",
    "test_data=preprocessing.text_dataset_from_directory(directory=test_dir,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 0  for the label corresponds to  csharp\n",
      "index 1  for the label corresponds to  java\n",
      "index 2  for the label corresponds to  javascript\n",
      "index 3  for the label corresponds to  python\n"
     ]
    }
   ],
   "source": [
    "for i,label in enumerate(train_data.class_names):\n",
    "    print('index' ,i,\" for the label corresponds to \", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\"blank clean up of (local) objects referenced in \"\"delayed\"\" functions does blank (pure, not jquery, if it matters) know to clear up/free/release from the last reference to an object in a \"\"delayed\"\" function called from a timer or event?..take the following code:..function myinitfunc().{.  var myinitobj = new object();.  myinitobj.properties = lotsofstuff;..  var mydelayedinitfunc = function ().  {.    dosomethingwith(myinitobj);.    // i shall not be accessing myinitobj again now..  };..  // let\\'s say, *one* of the following:.  settimeout(mydelayedinitfunc, 1000);.  window.addeventlistener(\\'load\\', mydelayedinitfunc);.  document.addeventlistener(\\'domcontentloaded\\', mydelayedinitfunc);.}...note that mydelayedinitfunc() is deliberately accessing variable myinitobj, which is local to myinitfunc()...in, say, http://blank.info/tutorial/memory-leaks it states \"\"functions used in settimeout/setinterval are also referenced internally and tracked until complete, then cleaned up\"\".  does this \"\"clean up\"\" understand that it can get rid of the myinitobj as well as the function itself?  i\\'m sort of guessing it does......what about the two event examples?  even though we know they are \"\"one-shot\"\" events, i\\'m guessing that neither mydelayedinitfunc nor myinitobj will get cleaned up?..if it is the case that some of these do not clean up, should i make mydelayedfunc() set myinitobj = null; at its end so as to minimise the wastage?\"\\n'\n",
      " \n",
      " \n",
      "\n",
      "csharp\n",
      " \n",
      " \n",
      " \n",
      "b'\"calculate percentage if percentage sign is in the text my project has a textbox field which is named as txtdisc. i need to minus the entered value from another textbox called txttotal. the problem is i need to minus the percentage if i enter the percentage sign. for example, if my total value is 1000 and my discount value is 2 then i need the answer as 998 and if the entered value is 2% i need the value as 980. i used _textchanged event for the calculation...my code is:..private void txtdiscount_textchanged(object sender, eventargs e).{.  if (txtdiscount.text.length &gt; 0 &amp;&amp; lbltotal.text != \"\"\"\").  {.    decimal net = 0, total = 0, discount = 0;.    total = convert.todecimal(lbltotal.text);.    discount = convert.todecimal(txtdiscount.text);.    net =total- discount;.    lblnetamount.text = net.tostring();.  }.}\"\\n'\n",
      " \n",
      " \n",
      "\n",
      "csharp\n",
      " \n",
      " \n",
      " \n",
      "b'\"what is the next code mean, the \\'lambda request\\' and the \\'**kwargs: {}\\',i have never see this def validate(request, *args, **kwargs):.    form_class = kwargs.pop(\\'form_class\\').    extra_args_func = kwargs.pop(\\'callback\\', lambda request, *args, **kwargs: {})...thanks....a={\\'a\\':\\'aaa\\',\\'b\\':\\'bbb\\'}.b=a.pop(\\'a\\',lambda x,y:x).print a...i know dict.pop(\\'a\\'),but i don\\'t know dict.pop(\\'a\\',func).what is the use of \\'func\\xe2\\x80\\x98 in here\"\\n'\n",
      " \n",
      " \n",
      "\n",
      "csharp\n",
      " \n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "for x,y in train_data.take(3):\n",
    "    for i in range(1):\n",
    "        X=(x.numpy()[i])\n",
    "        print(x.numpy()[i])\n",
    "        print(' \\n \\n')\n",
    "        print(train_data.class_names[i])\n",
    "        print(' \\n \\n ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first time the dataset is iterated over, its elements will be cached\n",
    "either in the specified file or in memory. Subsequent iterations will\n",
    "use the cached data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=train_data.cache().prefetch(tf.data.AUTOTUNE)\n",
    "val_data=val_data.cache().prefetch(tf.data.AUTOTUNE)\n",
    "train_data=test_data.cache().prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_br(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    lowercase=tf.strings.strip(lowercase)    \n",
    "    stripped_html = tf.strings.regex_replace(lowercase,\"<[^>]+>\" , '')\n",
    "    return tf.strings.regex_replace(stripped_html,\n",
    "                                  '[%s]' % re.escape(string.punctuation), '')\n",
    "max_features = 10000  # Maximum vocab size.\n",
    "max_tokens=150\n",
    "\n",
    "encode_input=TextVectorization(standardize=remove_br,\n",
    "                               max_tokens=max_features,output_mode='int',\n",
    "                               output_sequence_length=max_tokens\n",
    "                                )\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_input.adapt(train_data.map(lambda x,y:x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_dim=16\n",
    "class GRU(K.models.Model):\n",
    "    def __init__(self):\n",
    "        super(GRU,self).__init__()\n",
    "        self.embedd=K.layers.Embedding(input_dim=max_features,output_dim=embedded_dim,\n",
    "                                       input_length=max_tokens)\n",
    "        self.gru=K.layers.GRU(32)\n",
    "        self.f=K.layers.Flatten()\n",
    "        self.dense=K.layers.Dense(4,activation='softmax')\n",
    "        self.drop=K.layers.Dropout(0.3)\n",
    "    def call(self,x):\n",
    "        encoder=encode_input(x)\n",
    "        embedd=self.embedd(encoder)\n",
    "        gru=self.gru(embedd)\n",
    "        f=self.f(gru)\n",
    "        drop=self.drop(f)\n",
    "        output=self.dense(drop)\n",
    "        return output\n",
    "gru=GRU()\n",
    "# since the labels for each class are integers we will use  'sparse_categorical_crossentropy'\n",
    "# as the loss function\n",
    "gru.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "800/800 [==============================] - 71s 85ms/step - loss: 1.3860 - acc: 0.2509 - val_loss: 1.3639 - val_acc: 0.2950\n",
      "Epoch 2/20\n",
      "800/800 [==============================] - 62s 77ms/step - loss: 1.3003 - acc: 0.3479 - val_loss: 1.1453 - val_acc: 0.4408\n",
      "Epoch 3/20\n",
      "800/800 [==============================] - 62s 78ms/step - loss: 1.1769 - acc: 0.4030 - val_loss: 1.1166 - val_acc: 0.4400\n",
      "Epoch 4/20\n",
      "800/800 [==============================] - 73s 92ms/step - loss: 1.1417 - acc: 0.4381 - val_loss: 1.0757 - val_acc: 0.4758\n",
      "Epoch 5/20\n",
      "800/800 [==============================] - 65s 81ms/step - loss: 1.0799 - acc: 0.4812 - val_loss: 1.0137 - val_acc: 0.5108\n",
      "Epoch 6/20\n",
      "800/800 [==============================] - 58s 73ms/step - loss: 0.9559 - acc: 0.5981 - val_loss: 0.8584 - val_acc: 0.6208\n",
      "Epoch 7/20\n",
      "800/800 [==============================] - 60s 75ms/step - loss: 0.7942 - acc: 0.6945 - val_loss: 0.7689 - val_acc: 0.6775\n",
      "Epoch 8/20\n",
      "800/800 [==============================] - 64s 80ms/step - loss: 0.6798 - acc: 0.7394 - val_loss: 0.7326 - val_acc: 0.7083\n",
      "Epoch 9/20\n",
      "800/800 [==============================] - 62s 77ms/step - loss: 0.5955 - acc: 0.7783 - val_loss: 0.6897 - val_acc: 0.7367\n",
      "Epoch 10/20\n",
      "800/800 [==============================] - 61s 76ms/step - loss: 0.5248 - acc: 0.8100 - val_loss: 0.6346 - val_acc: 0.7583\n",
      "Epoch 11/20\n",
      "800/800 [==============================] - 59s 74ms/step - loss: 0.4754 - acc: 0.8249 - val_loss: 0.6441 - val_acc: 0.7725\n",
      "Epoch 12/20\n",
      "800/800 [==============================] - 57s 71ms/step - loss: 0.4385 - acc: 0.8458 - val_loss: 0.5959 - val_acc: 0.7858\n",
      "Epoch 13/20\n",
      "800/800 [==============================] - 64s 80ms/step - loss: 0.4086 - acc: 0.8606 - val_loss: 0.6042 - val_acc: 0.7917\n",
      "Epoch 14/20\n",
      "800/800 [==============================] - 63s 79ms/step - loss: 0.3796 - acc: 0.8675 - val_loss: 0.5901 - val_acc: 0.8008\n",
      "Epoch 15/20\n",
      "800/800 [==============================] - 62s 78ms/step - loss: 0.3525 - acc: 0.8818 - val_loss: 0.5976 - val_acc: 0.7817\n",
      "Epoch 16/20\n",
      "800/800 [==============================] - 60s 75ms/step - loss: 0.3331 - acc: 0.8890 - val_loss: 0.6508 - val_acc: 0.7767\n",
      "Epoch 17/20\n",
      "800/800 [==============================] - 62s 78ms/step - loss: 0.3125 - acc: 0.8987 - val_loss: 0.6696 - val_acc: 0.7825\n",
      "Epoch 18/20\n",
      "800/800 [==============================] - 61s 77ms/step - loss: 0.2909 - acc: 0.9032 - val_loss: 0.6899 - val_acc: 0.7783\n",
      "Epoch 19/20\n",
      "800/800 [==============================] - 62s 77ms/step - loss: 0.2752 - acc: 0.9098 - val_loss: 0.6880 - val_acc: 0.7800\n",
      "Epoch 20/20\n",
      "800/800 [==============================] - 62s 77ms/step - loss: 0.2579 - acc: 0.9171 - val_loss: 0.7575 - val_acc: 0.7842\n"
     ]
    }
   ],
   "source": [
    "history=gru.fit(train_data,batch_size=batch_size,epochs=20,validation_data=val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 22s 27ms/step - loss: 0.2477 - acc: 0.9210\n"
     ]
    }
   ],
   "source": [
    "loss,acc=gru.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "according the text data the answer for the 1st text is python , 2nd is javascript 3rd is java and\n",
    "4th=pyton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample=[\"variables keep changing back to their original value inside a while loop i am doing the mitx 6.00.01x course and i am on the second problem set on the 3rd problem and i am stuck. .my code:  ..    balance = 320000.    annualinterestrate = 0.2.    monthlyinterestrate = (annualinterestrate) / 12.0.    monthlyfixedpayment = 0.    empbalance = balance.    lowerbound = round((balance)/12,2).    upperbound = (balance*(1+monthlyinterestrate)**12)/12.    monthlyfixedpayment = round( ( (lowerbound+upperbound)/2) ,2).    while tempbalance != 0: .        monthlyfixedpayment = round( ( (lowerbound+upperbound)/2) ,2)  .        for m in range(12) :.            tempbalance -= monthlyfixedpayment .            tempbalance += (monthlyinterestrate)*(tempbalance).            tempbalance = round(tempbalance,2) .        if tempbalance &gt; 0:.            lowerbound = round(monthlyfixedpayment,2).            tempbalance = balance.        elif tempbalance &lt; 0: .            upperbound = round(monthlyfixedpayment,2).            tempbalance = balance..    print('lowest payment: ' + str(round(monthlyfixedpayment,2)))...my code uses bisection search to generate the monthlyfixedpayment but after i get to the lines at the end that changes the upperbound or lowerbound values and then start the loop again, the lowerbound and upperbound values reset to their values to the ones outside the loop. does anyone knows how to prevent this?\",\n",
    "        \"how pass window handler from one page to another? (blank) i have a very strange problem , please donâ€™t ask me why do i need thisâ€¦.i have a page1. page1 has a link which opens new window (page2) using  window.open function..chatwindow is a handler of child window with returns from window.open function..now i'm moving from page1 to page3 (by link &lt;a href=\"\"....\"\" target=\"\"_self\"\"&gt;some text&lt;/a&gt;). and i need to check on the page3 if page2 is close or open..how to pass handler chatwindow from page1 to page3?..thank you in advance!\",\n",
    "        \"what is the difference between text and string? in going through the blankfx tutorial i've run into the text, and it's being used where i would have thought a string would be used. is the only difference between..string foo = new string(\"\"bat\"\");...and..text bar = new text(\"\"bat\"\");...that bar cannot be edited, or are there other differences that i haven't been able to find in my research?\",\n",
    "        \"idiomatic blank iterating and adding to a dict i'm running through a string, creating all substrings of size 10, and adding them to a dict. this is my code,..sequence_map = {}.for i in range(len(s)):.    sub = s[i:i+10].    if sub in sequence_map:.       sequence_map[sub] += 1.    else:.       sequence_map[sub] = 1...is there a way to do this more blankically?..also how do i do the reverse blankically, as in interating through the dict and composing a list where value is equal to something?..[k for k, v in sequence_map.items()]\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(result):\n",
    "    for i in result:\n",
    "        if i==0:\n",
    "            print('csharp')\n",
    "        elif i==1:\n",
    "            print('java')\n",
    "        elif i==2:\n",
    "            print('javascript')\n",
    "        elif i==3:\n",
    "            print('python')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result=tf.argmax(gru.predict(sample)).numpy()\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "java\n",
      "javascript\n",
      "java\n",
      "csharp\n"
     ]
    }
   ],
   "source": [
    "pred(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> <b>References:</b></p>\n",
    "<a href='https://arxiv.org/pdf/1406.1078v3.pdf' target='_blank'>Gated Recurrent Unit or GRU (Cho et al., 2014)</a>\n",
    "\n",
    "<a href='https://d2l.ai/chapter_recurrent-modern/gru.html' target='_blank'\n",
    "title=\"Dive Into Deep Learning Chapter 9\">Gated Recurrent Units (GRU)</a>\n",
    "\n",
    "<a href='https://en.wikipedia.org/wiki/Gated_recurrent_unit' target='_blank'\n",
    "title=\"wikipedia\">Gated Recurrent Units</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
