{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#word-embeddings\" data-toc-modified-id=\"word-embeddings-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>word embeddings</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Word-embeddings\" data-toc-modified-id=\"Word-embeddings-1.0.1\"><span class=\"toc-item-num\">1.0.1&nbsp;&nbsp;</span>Word embeddings</a></span></li></ul></li></ul></li><li><span><a href=\"#The-Embedding-layer\" data-toc-modified-id=\"The-Embedding-layer-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>The Embedding layer</a></span></li><li><span><a href=\"#Text-preprocessing\" data-toc-modified-id=\"Text-preprocessing-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Text preprocessing</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to perform machine learning on text documents, the raw (text) data cannot be fed directly to algorithm as these algorithms expect numerical feature vectors so instead we need to turn the text content into numerical feature vectors.\n",
    "\n",
    "From the [scikit-learn documentation](https://scikit-learn.org/stable/modules/feature_extraction.html):\n",
    "<b>\n",
    "We call vectorization the general process of turning a collection of text documents into numerical feature vectors.\n",
    "</b>\n",
    "So Vectorizing text is the process of transforming text into numeric tensors. \n",
    "\n",
    "Vectorization can be done in multiple ways:\n",
    "- Segment text into words, and transform each word into a vector.\n",
    "- Segment text into characters, and transform each character into a vector.\n",
    "- Extract n-grams of words or characters, and transform each n-gram into a vector.\n",
    "N-grams are overlapping groups of multiple consecutive words or characters\n",
    "\n",
    "<b>Tokenization: </b> the segementation of text into words or characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Text-vectorization processes consist of applying some tokenization scheme to the text then associating numeric vectors with the generated tokens.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another popular and powerful way to associate a vector with a word is the use of dense\n",
    "word vectors, also called word embeddings. Unlike the one-hot encoding, word embeddings are learned from data.\n",
    "\n",
    "\n",
    "<P>Consider the sentence \"The cat sat on the mat\". The vocabulary (or unique words) in this sentence is (cat, mat, on, sat, the). </P>\n",
    "From <a href=\"https://www.tensorflow.org/tutorials/text/word_embeddings\">Tensorflow documentation-Word embeddings</a>\n",
    "\n",
    "#### Word embeddings\n",
    "\n",
    "> Word embeddings give us a way to use an efficient, dense representation in which similar words have a similar encoding. Importantly, you do not have to specify this encoding by hand. An embedding is a dense vector of floating point values (the length of the vector is a parameter you specify). Instead of specifying the values for the embedding manually, they are trainable parameters (weights learned by the model during training, in the same way a model learns weights for a dense layer). It is common to see word embeddings that are 8-dimensional (for small datasets), up to 1024-dimensions when working with large datasets. A higher dimensional embedding can capture fine-grained relationships between words, but takes more data to learn.\n",
    "<img src=\"images/embedding2.png?raw=1\" alt=\"Diagram of an embedding\" width=\"400\"/>\n",
    "Above is a diagram for a word embedding. Each word is represented as a 4-dimensional vector of floating point values. Another way to think of an embedding is as \"lookup table\". After these weights have been learned, you can encode each word by looking up the dense vector it corresponds to in the table.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## The Embedding layer\n",
    "From <a href=\"https://www.tensorflow.org/tutorials/text/word_embeddings\">Tensorflow documentation-Word embeddings</a>\n",
    "> Keras makes it easy to use word embeddings. Take a look at the Embedding layer.\n",
    "The Embedding layer can be understood as a lookup table that maps from integer indices (which stand for specific words) to dense vectors (their embeddings). The dimensionality (or width) of the embedding is a parameter you can experiment with to see what works well for your problem, much in the same way you would experiment with the number of neurons in a Dense layer.\n",
    "\n",
    "> When you create an Embedding layer, the weights for the embedding are randomly initialized (just like any other layer). During training, they are gradually adjusted via backpropagation. Once trained, the learned word embeddings will roughly encode similarities between words (as they were learned for the specific problem your model is trained on). If you pass an integer to an embedding layer, the result replaces each integer with the vector from the embedding table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding,Layer,Flatten,Dense,Dropout\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "import os,re,string\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For text or sequence problems, the Embedding layer takes as input a 2D tensor of integers, of shape (samples,sequence_length) and returns a 3D floating-point tensor of shape (samples, sequence_length, embedding_dimensionality). All sequences in a batch must have the same length, though (because you need to pack them into a single tensor), so sequences that are shorter than others should be padded with zeros, and sequences that are longer should be truncated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer=Embedding(10,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.02173385, -0.01164292, -0.01864962, -0.04104736,\n",
       "         -0.03320467],\n",
       "        [ 0.02778766,  0.01905247,  0.0294116 ,  0.02264091,\n",
       "         -0.03183525]],\n",
       "\n",
       "       [[ 0.02778766,  0.01905247,  0.0294116 ,  0.02264091,\n",
       "         -0.03183525],\n",
       "        [ 0.00019507,  0.0349418 ,  0.0323015 , -0.03063896,\n",
       "          0.00229504]]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = embedding_layer(tf.constant([[1,2],[2,4]]))\n",
    "result.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the IMDB data for use with an Embedding layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ll restrict the movie reviews to the top 15,000 most common words and  considering looking at the first 30 words in every review. The network will learn 16-dimensional embeddings for each of the 15,000 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeledBow.feat\n",
      "neg\n",
      "pos\n",
      "unsupBow.feat\n",
      "urls_neg.txt\n",
      "urls_pos.txt\n",
      "urls_unsup.txt\n"
     ]
    }
   ],
   "source": [
    "!ls  './data/aclImdb/train/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir=os.path.dirname('./data/aclImdb/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/aclImdb'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 18750 files for training.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Using 6250 files for validation.\n"
     ]
    }
   ],
   "source": [
    "batch_size=100\n",
    "seed = 100\n",
    "train_data=tf.keras.preprocessing.text_dataset_from_directory('./data/aclImdb/train/',\n",
    "                                                            batch_size=batch_size,\n",
    "                                                              validation_split=0.25,\n",
    "                                                          subset='training',\n",
    "                                                              seed=seed)\n",
    "\n",
    "val_data=tf.keras.preprocessing.text_dataset_from_directory('./data/aclImdb/train/',\n",
    "                                                            batch_size=batch_size,\n",
    "                                                           validation_split=0.25,\n",
    "                                                            subset='validation', seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above result, there are 25,000 examples in the training folder, of which 75% (18750) is used for training and 25% (6250) as a validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# names of the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 0 corresponds to  neg\n",
      "index 1 corresponds to  pos\n"
     ]
    }
   ],
   "source": [
    "for i,label in enumerate(train_data.class_names):\n",
    "    print('index' ,i,\"corresponds to \", label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates a `Dataset` that prefetches elements from this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE=tf.data.AUTOTUNE\n",
    "batch_size = 1000\n",
    "train_data=train_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "#val_data=val_data.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"Neither the total disaster the UK critics claimed nor the misunderstood masterpiece its few fanboys insist, Revolver is at the very least an admirable attempt by Guy Ritchie to add a little substance to his conman capers. But then, nothing is more despised than an ambitious film that bites off more than it can chew, especially one using the gangster/con-artist movie framework. As might be expected from Luc Besson's name on the credits as producer, there's a definite element of 'Cinema de look' about it: set in a kind of realistic fantasy world where America and Britain overlap, it looks great, has a couple of superbly edited and conceived action sequences and oozes style, all of which mark it up as a disposable entertainment. But Ritchie clearly wants to do more than simply rehash his own movies for a fast buck, and he's spent a lot of time thinking and reading about life, the universe and everything. If anything its problem is that he's trying to throw in too many influences (a bit of Machiavelli, a dash of Godard, a lot of the Principles of Chess), motifs and techniques, littering the screen with quotes: the film was originally intended to end with three minutes of epigrams over photos of corpses of mob victims, and at times it feels as if he never read a fortune cookie he didn't want to turn into a movie. Rather than a commercial for Kabbalism, it's really more a mixture of the overlapping principles of commerce, chess and confidence trickery that for the most part pulls off the difficult trick of making the theosophy accessible while hiding the film's central (somewhat metaphysical) con.<br /><br />The last third is where most of the problems can be found as Jason Statham takes on the enemy (literally) within with lots of ambitious but not always entirely successful crosscutting within the frame to contrast people's exterior bravado with their inner fear and anger, but it's got a lot going for it all the same. Not worth starting a new religion over, but I'm surprised it didn't get a US distributor. Maybe they found Ray Liotta's intentionally fake tan just too damn scary?\" \n",
      " \n",
      " 1\n"
     ]
    }
   ],
   "source": [
    "for x,y in train_data.take(1):\n",
    "    for i in range(1):\n",
    "        print(x[i].numpy(),'\\n \\n',y[i].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing\n",
    "\n",
    "You’ll restrict the movie reviews to the top 15,000 most common words and will consider looking at the first 30 words in every review. The network will learn 16-dimensional embeddings for each of the 15,000 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_br(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, '<br />', '')\n",
    "    return tf.strings.regex_replace(stripped_html,\n",
    "                                  '[%s]' % re.escape(string.punctuation), '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features=15000\n",
    "embedding_dim=16\n",
    "max_length=30\n",
    "encoder=TextVectorization(max_tokens=max_features,standardize=remove_br,\n",
    "                          output_mode='int',\n",
    "                          output_sequence_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.adapt(train_data.map(lambda x,y:x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .adapt method sets the layer's vocabulary. Here are the first 70 tokens. After the padding and unknown tokens they're sorted by frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['', '[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it',\n",
       "       'this', 'i', 'that', 'was', 'as', 'with', 'for', 'movie', 'but',\n",
       "       'film', 'on', 'not', 'you', 'are', 'his', 'have', 'he', 'be',\n",
       "       'one', 'its', 'at', 'all', 'by', 'an', 'they', 'from', 'who',\n",
       "       'like', 'so', 'her', 'or', 'just', 'about', 'has', 'out', 'if',\n",
       "       'some', 'what', 'there', 'good', 'more', 'very', 'when', 'she',\n",
       "       'even', 'no', 'up', 'would', 'my', 'only', 'time', 'which',\n",
       "       'really', 'story', 'their', 'were', 'had', 'see', 'can', 'me'],\n",
       "      dtype='<U17')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab=np.array(encoder.get_vocabulary())\n",
    "vocab[:70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoder.get_vocabulary())==max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(encoder)\n",
    "model.add(Embedding(input_dim=max_features,output_dim=embedding_dim,input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(14,activation='relu'))\n",
    "model.add(Dropout(0.05))\n",
    "model.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "188/188 [==============================] - 19s 88ms/step - loss: 0.6904 - acc: 0.5340 - val_loss: 0.6472 - val_acc: 0.6685\n",
      "Epoch 2/4\n",
      "188/188 [==============================] - 13s 67ms/step - loss: 0.5877 - acc: 0.7305 - val_loss: 0.5384 - val_acc: 0.7282s - loss: 0.5886 - acc: 0.73\n",
      "Epoch 3/4\n",
      "188/188 [==============================] - 13s 69ms/step - loss: 0.4302 - acc: 0.8146 - val_loss: 0.5260 - val_acc: 0.7357\n",
      "Epoch 4/4\n",
      "188/188 [==============================] - 12s 66ms/step - loss: 0.3269 - acc: 0.8654 - val_loss: 0.5533 - val_acc: 0.7341\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_data,epochs=4,batch_size=32,validation_data=val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative review\n"
     ]
    }
   ],
   "source": [
    "# predict on a sample text without padding.\n",
    "\n",
    "sample_text = ('the animation and graphics were very good but The movie was too bad'\n",
    "               ' so am confused whether to recommend this movie or not.')\n",
    "\n",
    "predictions = model.predict(np.array([sample_text]))\n",
    "result='positive review' if predictions>0.5 else 'negative review'\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive review\n",
      "negative review\n",
      "negative review\n"
     ]
    }
   ],
   "source": [
    "# predict on a sample text without padding.\n",
    "\n",
    "text = [\n",
    "  \"The movie was great!\",\n",
    "  \"This is a confused movie.\",\n",
    "  \"The movie was terrible...\"\n",
    "]\n",
    "for i in text:\n",
    "    predictions = model.predict(np.array([i]))\n",
    "    result='positive review' if predictions>0.5 else 'negative review'\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
