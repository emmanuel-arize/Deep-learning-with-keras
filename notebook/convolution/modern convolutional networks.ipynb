{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#LENET-NETWORK\" data-toc-modified-id=\"LENET-NETWORK-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>LENET NETWORK</a></span></li><li><span><a href=\"#ALEXNET\" data-toc-modified-id=\"ALEXNET-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>ALEXNET</a></span></li><li><span><a href=\"#VGG-Blocks\" data-toc-modified-id=\"VGG-Blocks-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>VGG Blocks</a></span></li><li><span><a href=\"#Network-in-Network-(NiN)-BLOCK\" data-toc-modified-id=\"Network-in-Network-(NiN)-BLOCK-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Network in Network (NiN) BLOCK</a></span></li><li><span><a href=\"#INCEPTION\" data-toc-modified-id=\"INCEPTION-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>INCEPTION</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import tensorflow_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LENET NETWORK\n",
    "<img src='../images/lenet.svg'>\n",
    "<img src='../images/lenet.jpg'>\n",
    "\n",
    "\n",
    " (source: Dive into Deep Learning by Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola page 252-3)\n",
    "  \n",
    "\n",
    "To read more on LENET visit :\n",
    "\n",
    "<a href='http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf'>GradientBased Learning Applied to Document Recognition</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lenet(keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cov1=layers.Conv2D(filters=6,kernel_size=5,padding='same',activation='sigmoid')\n",
    "        self.avg1=layers.AvgPool2D(pool_size=2,strides=2)\n",
    "        self.cov2=layers.Conv2D(filters=16,kernel_size=5,activation='sigmoid')\n",
    "        self.avg2=layers.AvgPool2D(pool_size=2,strides=2)\n",
    "        self.f=layers.Flatten()\n",
    "        self.dense1=layers.Dense(120,activation='sigmoid')\n",
    "        self.dense2=layers.Dense(84,activation='sigmoid')\n",
    "        self.dense3=layers.Dense(10)\n",
    "    def call(self,x):\n",
    "        h1=self.avg1(self.cov1(x))\n",
    "        h2=self.avg2(self.cov2(h1))\n",
    "        h2=self.f(h2)\n",
    "        output=self.dense3(self.dense2(self.dense1(h2)))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As compared to the original network, we took the liberty of replacing the Gaussian activation in\n",
    "the last layer by a regular dense layer, which tends to be significantly more convenient to train.\n",
    "Other than that, this network matches the historical definition of LeNet5.\n",
    "Next, let us take a look of an example. As shown in Fig. 6.6.2, we feed a single-channel example\n",
    "of size 28 Ã— 28 into the network and perform a forward computation layer by layer printing the\n",
    "output shape at each layer to make sure we understand what is happening here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in keras the first convolution layer takes a feature map of size <b>(batch_size , height , width , depth axis)</b >which represent the height, width and the depth axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet=Lenet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2D output shape: \t (1, 28, 28, 6)\n",
      "AveragePooling2D output shape: \t (1, 14, 14, 6)\n",
      "Conv2D output shape: \t (1, 10, 10, 16)\n",
      "AveragePooling2D output shape: \t (1, 5, 5, 16)\n",
      "Flatten output shape: \t (1, 400)\n",
      "Dense output shape: \t (1, 120)\n",
      "Dense output shape: \t (1, 84)\n",
      "Dense output shape: \t (1, 10)\n"
     ]
    }
   ],
   "source": [
    "X=tf.random.uniform(shape=(1, 28, 28,1))\n",
    "for layer in lenet.layers:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__, 'output shape: \\t', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALEXNET\n",
    "<img src='../images/alexnet.jpg'>\n",
    " (source: Dive into Deep Learning by Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola page 261)\n",
    "  \n",
    "To read more on Alexnet visit :\n",
    "\n",
    "<a href='https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf'>ImageNet Classification with Deep Convolutional\n",
    "Neural Networks\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alexnet():\n",
    "    return tf.keras.models.Sequential([\n",
    "        layers.Conv2D(filters=96,kernel_size=11,strides=4,activation='relu'),\n",
    "        layers.MaxPool2D(pool_size=3,strides=2),\n",
    "        layers.Conv2D(filters=256,kernel_size=5,padding='same',activation='relu'),\n",
    "        layers.MaxPool2D(pool_size=3,strides=2),\n",
    "        layers.Conv2D(filters=384,kernel_size=3,padding='same',activation='relu'),\n",
    "        layers.Conv2D(filters=384,kernel_size=3,padding='same',activation='relu'),\n",
    "        layers.Conv2D(filters=384,kernel_size=3,padding='same',activation='relu'),\n",
    "        layers.MaxPool2D(pool_size=3,strides=2),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(4096,activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(4096,activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(10)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We construct a single-channel data instance with both height and width of 224 to observe the output shape of each layer. It matches our diagram above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2D output shape:\t (1, 54, 54, 96)\n",
      "MaxPooling2D output shape:\t (1, 26, 26, 96)\n",
      "Conv2D output shape:\t (1, 26, 26, 256)\n",
      "MaxPooling2D output shape:\t (1, 12, 12, 256)\n",
      "Conv2D output shape:\t (1, 12, 12, 384)\n",
      "Conv2D output shape:\t (1, 12, 12, 384)\n",
      "Conv2D output shape:\t (1, 12, 12, 384)\n",
      "MaxPooling2D output shape:\t (1, 5, 5, 384)\n",
      "Flatten output shape:\t (1, 9600)\n",
      "Dense output shape:\t (1, 4096)\n",
      "Dropout output shape:\t (1, 4096)\n",
      "Dense output shape:\t (1, 4096)\n",
      "Dropout output shape:\t (1, 4096)\n",
      "Dense output shape:\t (1, 10)\n"
     ]
    }
   ],
   "source": [
    "X = tf.random.uniform(shape=(1, 224, 224,1))\n",
    "for layer in alexnet().layers:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__, 'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG Blocks\n",
    "<img src='../images/vvg.jpg'>\n",
    "The function takes two arguments corresponding to the number of convolutional layers num_convs and the number of output channels num_channels\n",
    "\n",
    "\n",
    "To read more on VGG visit :\n",
    "\n",
    "<a href='https://arxiv.org/pdf/1409.1556.pdf'>VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../images/vgg.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original VGG network had 5 convolutional blocks, among which the first two have one convolutional layer each and the latter three contain two convolutional layers each but we will implement the vgg diagram above of which the first two have two convolutional layer each and the latter three contain three convolutional layers each.\n",
    "\n",
    "The first block has 64 output channels and each subsequent block doubles the number of output channels, until that number reaches 512. Since this network uses 8 convolutional layers and 3 fully-connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_1():\n",
    "    return tf.keras.models.Sequential([\n",
    "        layers.Conv2D(filters=64,kernel_size=3,padding='same',activation='relu'),\n",
    "        layers.Conv2D(filters=64,kernel_size=3,padding='same',activation='relu'),\n",
    "        layers.MaxPool2D(pool_size=2,strides=2),\n",
    "        \n",
    "        layers.Conv2D(128,kernel_size=3,padding='same',activation='relu'),\n",
    "        layers.Conv2D(128,kernel_size=3,padding='same',activation='relu'),\n",
    "        layers.MaxPool2D(pool_size=2,strides=2),\n",
    "        \n",
    "        layers.Conv2D(256,kernel_size=3,padding='same',activation='relu'),\n",
    "        layers.Conv2D(256,kernel_size=3,padding='same',activation='relu'),\n",
    "        layers.Conv2D(256,kernel_size=3,padding='same',activation='relu'),\n",
    "        layers.MaxPool2D(pool_size=2,strides=2),\n",
    "        \n",
    "        layers.Conv2D(512,kernel_size=3,padding='same',activation='relu'),\n",
    "        layers.Conv2D(512,kernel_size=3,padding='same',activation='relu'),\n",
    "        layers.Conv2D(512,kernel_size=3,padding='same',activation='relu'),\n",
    "        layers.MaxPool2D(pool_size=2,strides=2),\n",
    "        \n",
    "        layers.Conv2D(512,kernel_size=3,padding='same',activation='relu'),\n",
    "        layers.Conv2D(512,kernel_size=3,padding='same',activation='relu'),\n",
    "        layers.Conv2D(512,kernel_size=3,padding='same',activation='relu'),\n",
    "        layers.MaxPool2D(pool_size=2,strides=2),\n",
    "        \n",
    "        layers.Flatten(),\n",
    "        layers.Dense(4096,activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(4096,activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(10)\n",
    "        \n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Or implemented as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_block(num_convs,filters):\n",
    "    blk=tf.keras.models.Sequential()\n",
    "    for _ in range(num_convs):\n",
    "        blk.add(layers.Conv2D(filters,kernel_size=3,padding='same',activation='relu'))\n",
    "    blk.add(layers.MaxPool2D(pool_size=2,strides=2))\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_arch = ((2, 64), (2, 128), (3, 256), (3, 512), (3, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg(conv_arch):\n",
    "    blk=tf.keras.models.Sequential()\n",
    "    for (num_convs,filters) in conv_arch:\n",
    "        blk.add(vgg_block(num_convs,filters))\n",
    "    blk.add(layers.Flatten())\n",
    "    blk.add(layers.Dense(4096,activation='relu'))\n",
    "    blk.add(layers.Dropout(0.5))\n",
    "    blk.add(layers.Dense(4096,activation='relu'))\n",
    "    blk.add(layers.Dropout(0.5))\n",
    "    blk.add(layers.Dense(10))\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will construct a single-channel data example with a height and width of 224 to observe\n",
    "the output shape of each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_2 = vgg(conv_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2D output shape:\t (1, 224, 224, 64)\n",
      "Conv2D output shape:\t (1, 224, 224, 64)\n",
      "MaxPooling2D output shape:\t (1, 112, 112, 64)\n",
      "Conv2D output shape:\t (1, 112, 112, 128)\n",
      "Conv2D output shape:\t (1, 112, 112, 128)\n",
      "MaxPooling2D output shape:\t (1, 56, 56, 128)\n",
      "Conv2D output shape:\t (1, 56, 56, 256)\n",
      "Conv2D output shape:\t (1, 56, 56, 256)\n",
      "Conv2D output shape:\t (1, 56, 56, 256)\n",
      "MaxPooling2D output shape:\t (1, 28, 28, 256)\n",
      "Conv2D output shape:\t (1, 28, 28, 512)\n",
      "Conv2D output shape:\t (1, 28, 28, 512)\n",
      "Conv2D output shape:\t (1, 28, 28, 512)\n",
      "MaxPooling2D output shape:\t (1, 14, 14, 512)\n",
      "Conv2D output shape:\t (1, 14, 14, 512)\n",
      "Conv2D output shape:\t (1, 14, 14, 512)\n",
      "Conv2D output shape:\t (1, 14, 14, 512)\n",
      "MaxPooling2D output shape:\t (1, 7, 7, 512)\n",
      "Flatten output shape:\t (1, 25088)\n",
      "Dense output shape:\t (1, 4096)\n",
      "Dropout output shape:\t (1, 4096)\n",
      "Dense output shape:\t (1, 4096)\n",
      "Dropout output shape:\t (1, 4096)\n",
      "Dense output shape:\t (1, 10)\n"
     ]
    }
   ],
   "source": [
    "X = tf.random.uniform(shape=(1, 224, 224,1))\n",
    "for layer in vgg_1().layers:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__, 'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential output shape:\t (1, 112, 112, 64)\n",
      "Sequential output shape:\t (1, 56, 56, 128)\n",
      "Sequential output shape:\t (1, 28, 28, 256)\n",
      "Sequential output shape:\t (1, 14, 14, 512)\n",
      "Sequential output shape:\t (1, 7, 7, 512)\n",
      "Flatten output shape:\t (1, 25088)\n",
      "Dense output shape:\t (1, 4096)\n",
      "Dropout output shape:\t (1, 4096)\n",
      "Dense output shape:\t (1, 4096)\n",
      "Dropout output shape:\t (1, 4096)\n",
      "Dense output shape:\t (1, 10)\n"
     ]
    }
   ],
   "source": [
    "X = tf.random.uniform(shape=(1, 224, 224,1))\n",
    "for layer in vgg_2.layers:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__, 'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Network in Network (NiN) BLOCK\n",
    "<img src='../images/nin.jpg'/>\n",
    "The NiN block consists of one convolutional layer followed by two 1 Ã— 1 convolutional layers that\n",
    "act as per-pixel fully-connected layers with ReLU activations. The convolution width of the first\n",
    "layer is typically set by the user. The subsequent widths are fixed to 1 Ã— 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nin_block(filters, kernel_size, strides,padding):\n",
    "    return tf.keras.models.Sequential([\n",
    "        layers.Conv2D(filters,kernel_size,strides,padding,activation='relu'),\n",
    "        layers.Conv2D(filters,kernel_size=1,activation='relu'),\n",
    "         layers.Conv2D(filters,kernel_size=1,activation='relu')    \n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def nin():\n",
    "    return tf.keras.models.Sequential([\n",
    "        nin_block(filters=96,kernel_size=11,strides=4,padding='valid'),\n",
    "        layers.MaxPool2D(pool_size=3,strides=2),\n",
    "        \n",
    "        nin_block(filters=256,kernel_size=5,strides=1,padding='same'),\n",
    "        layers.MaxPool2D(pool_size=3,strides=2),\n",
    "        \n",
    "        nin_block(filters=384,kernel_size=5,strides=1,padding='same'),\n",
    "        layers.MaxPool2D(pool_size=3, strides=2),\n",
    "        layers.Dropout(0.5),\n",
    "        \n",
    "        nin_block(filters=10,kernel_size=3,strides=1,padding='same'),\n",
    "        layers.GlobalAvgPool2D(),\n",
    "        tf.keras.layers.Reshape((1, 1, 10)),\n",
    "        layers.Flatten()\n",
    "        \n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential output shape:\t (1, 54, 54, 96)\n",
      "MaxPooling2D output shape:\t (1, 26, 26, 96)\n",
      "Sequential output shape:\t (1, 26, 26, 256)\n",
      "MaxPooling2D output shape:\t (1, 12, 12, 256)\n",
      "Sequential output shape:\t (1, 12, 12, 384)\n",
      "MaxPooling2D output shape:\t (1, 5, 5, 384)\n",
      "Dropout output shape:\t (1, 5, 5, 384)\n",
      "Sequential output shape:\t (1, 5, 5, 10)\n",
      "GlobalAveragePooling2D output shape:\t (1, 10)\n",
      "Reshape output shape:\t (1, 1, 1, 10)\n",
      "Flatten output shape:\t (1, 10)\n"
     ]
    }
   ],
   "source": [
    "X = tf.random.uniform(shape=(1, 224, 224,1))\n",
    "for layer in nin().layers:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__, 'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INCEPTION\n",
    "<img src=\"../images/inception.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GoogLeNet uses a stack of a total of 9 inception blocks and global average pooling to generate its estimates. Maximum pooling between inception blocks reduced the\n",
    "dimensionality. The first part is identical to AlexNet and LeNet, the stack of blocks is inherited\n",
    "from VGG and the global average pooling avoids a stack of fully-connected layers at the end. The\n",
    "architecture is depicted below\n",
    "<img src=\"../images/inception1.jpg\" />\n",
    "<img src=\"../images/inception2.jpg\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception_block(tf.keras.Model):\n",
    "    def __init__(self,c1,c2,c3,c4,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # Path 1 is a single 1 x 1 convolutional layer\n",
    "        self.p1_1=layers.Conv2D(filters=c1,kernel_size=1,activation='relu')\n",
    "        # Path 2 is a 1 x 1 convolutional layer followed by a 3 x 3\n",
    "        # convolutional layer\n",
    "        self.p2_1=layers.Conv2D(filters=c2[0],kernel_size=1,activation='relu')\n",
    "        self.p2_2=layers.Conv2D(filters=c2[1],kernel_size=3,padding='same',activation='relu')\n",
    "         # Path 2 is a 1 x 1 convolutional layer followed by a 5 x 5\n",
    "        # convolutional layer\n",
    "        self.p3_1=layers.Conv2D(filters=c3[0],kernel_size=1,activation='relu')\n",
    "        self.p3_2=layers.Conv2D(filters=c3[1],kernel_size=5,padding='same',activation='relu')\n",
    "         # Path 4 is a 3 x 3 maximum pooling layer followed by a 1 x 1\n",
    "        # convolutional layer\n",
    "        self.p4_1=layers.MaxPool2D(pool_size=3,padding='same',strides=1)\n",
    "        self.p4_2=layers.Conv2D(filters=c4,kernel_size=1,activation='relu')\n",
    "    def call(self,x):\n",
    "        p1=self.p1_1(x)\n",
    "        p2=self.p2_2(self.p2_1(x))\n",
    "        p3=self.p3_2(self.p3_1(x))\n",
    "        p4=self.p4_2(self.p4_1(x))\n",
    "        return layers.Concatenate()([p1, p2, p3, p4])\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception():\n",
    "    return tf.keras.models.Sequential([\n",
    "        layers.Conv2D(filters=64,kernel_size=7,strides=2,padding='same',activation='relu'),\n",
    "        layers.MaxPool2D(pool_size=3,padding='same',strides=2),\n",
    "        \n",
    "        layers.Conv2D(64,kernel_size=1,activation='relu'),\n",
    "        layers.Conv2D(192,kernel_size=3,padding='same',activation='relu'),\n",
    "        layers.MaxPool2D(pool_size=3,padding='same',strides=2),\n",
    "        # inception(3a)\n",
    "        Inception_block(c1=64,c2=(96,128),c3=(16,32),c4=32),\n",
    "               # inception(3b)\n",
    "        Inception_block(c1=128,c2=(128,192),c3=(32,96),c4=64),\n",
    "        layers.MaxPool2D(pool_size=3,padding='same',strides=2),\n",
    "        # inception(4a)\n",
    "        Inception_block(c1=192,c2=(96,208),c3=(16,48),c4=64),\n",
    "              # inception(4b)\n",
    "        Inception_block(c1=160,c2=(112,224),c3=(24,64),c4=64),\n",
    "        # inception(4c)\n",
    "        Inception_block(c1=128,c2=(128,256),c3=(24,64),c4=64),\n",
    "        # inception(4d)\n",
    "        Inception_block(112,(144,288),(32,64),64),\n",
    "        # inception(4e)\n",
    "        Inception_block(256,(160,320),(32,128),128),\n",
    "        layers.MaxPool2D(pool_size=3,padding='same',strides=2),\n",
    "         # inception(5a)\n",
    "        Inception_block(256, (160, 320), (32, 128), 128),\n",
    "              # inception(5b)\n",
    "        Inception_block(384, (192, 384), (48, 128), 128),\n",
    "        layers.MaxPool2D(pool_size=3,padding='same',strides=2),\n",
    "        layers.GlobalAvgPool2D(),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(10,activation='softmax')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2D output shape:\t (1, 1, 48, 64)\n",
      "MaxPooling2D output shape:\t (1, 1, 24, 64)\n",
      "Conv2D output shape:\t (1, 1, 24, 64)\n",
      "Conv2D output shape:\t (1, 1, 24, 192)\n",
      "MaxPooling2D output shape:\t (1, 1, 12, 192)\n",
      "Inception_block output shape:\t (1, 1, 12, 256)\n",
      "Inception_block output shape:\t (1, 1, 12, 480)\n",
      "MaxPooling2D output shape:\t (1, 1, 6, 480)\n",
      "Inception_block output shape:\t (1, 1, 6, 512)\n",
      "Inception_block output shape:\t (1, 1, 6, 512)\n",
      "Inception_block output shape:\t (1, 1, 6, 512)\n",
      "Inception_block output shape:\t (1, 1, 6, 528)\n",
      "Inception_block output shape:\t (1, 1, 6, 832)\n",
      "MaxPooling2D output shape:\t (1, 1, 3, 832)\n",
      "Inception_block output shape:\t (1, 1, 3, 832)\n",
      "Inception_block output shape:\t (1, 1, 3, 1024)\n",
      "MaxPooling2D output shape:\t (1, 1, 2, 1024)\n",
      "GlobalAveragePooling2D output shape:\t (1, 1024)\n",
      "Flatten output shape:\t (1, 1024)\n",
      "Dropout output shape:\t (1, 1024)\n",
      "Dense output shape:\t (1, 10)\n"
     ]
    }
   ],
   "source": [
    "X = tf.random.uniform(shape=(1, 1, 96, 96))\n",
    "for layer in inception().layers:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__, 'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
